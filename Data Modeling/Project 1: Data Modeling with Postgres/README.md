# __Project 1: Data Modeling with Postgres__

## __Project Summary__
### Scenario
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

### Data
There are two datasets:
##### Songs Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### Database
The database to be created in this project should have a star schema like the one described below.
_Fact table_
- Songplays
    -- songplay_id SERIAL PRIMARY KEY
    -- start_time TIMESTAMP NOT NULL
    -- user_id int NOT NULL
    -- level varchar
    -- song_id varchar
    -- artist_id varchar
    -- session_id varchar
    -- location text
    -- user_agent text
    
_Dimention tables_
- Users
    -- user_id int PRIMARY KEY
    -- first_name varchar
    -- last_name varchar
    -- gender varchar
    -- level varchar
- Songs
    -- song_id varchar PRIMARY KEY
    -- title text NOT NULL
    -- artist_id varchar NOT NULL
    -- year int
    -- duration float NOT NULL
- Artists
    -- artist_id varchar PRIMARY KEY
    -- name text NOT NULL
    -- location text
    -- latitude float
    -- longitude float
- Time
    -- start_time TIMESTAMP PRIMARY KEY
    -- hour int
    -- day int
    -- week int
    -- month int
    -- year int
    -- weekday varchar

## __Project Structure__
This project is composed of the following elements:
- data: Folder containing the data used for creating the tables;
- create_tables.py: Script to reset the tables;
- etl.ipynb: notebook to test the functionality to be implemented in the etl script;
- etl.py: Script that implements the the pipeline that reads the data files and creates the tables in the database;
- sql_queries.py: Script containing the queries used to create the tables and insert the reccords into the them;
- test.ipynb: notebook that runs a series of simple tests and sanity checks that validate that the etl process is working as inteded;
- README.md: README file for the project.

## __Running the Scripts__
In order to run the etl process that creates the tables in the database, follow the steps bellow:
1. Run create_tables.py in order to reset the tables in the database:
    `python create_tables.py`
2. Run the etl.py script in order to process the data files and create the tables:
    `python etl.py`
3. Run the test.ipynb for a quick set of validations:
    - Restart the Kernel;
    - Run all cells.


